{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "128fd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "载入需要用到的包和数据\n",
    "\"\"\"\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ab5e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = idx2numpy.convert_from_file('./DataSets/mnist/train-images.idx3-ubyte')\n",
    "y_train = idx2numpy.convert_from_file('./DataSets/mnist/train-labels.idx1-ubyte')\n",
    "x_test = idx2numpy.convert_from_file('./DataSets/mnist/t10k-images.idx3-ubyte')\n",
    "y_test = idx2numpy.convert_from_file('./DataSets/mnist/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7d4b00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n获得数据，并对因变量进行独热编码\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "获得数据，并对因变量进行独热编码\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc744111",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa0c4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,train_labels = (x_train/255,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57853b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据的独热码\n",
    "one_hot_labels = np.zeros((len(train_labels),10))\n",
    "for i,j in enumerate(train_labels):\n",
    "    one_hot_labels[i][j] = 1\n",
    "    # print(f\"i:{i}, j:{j}\")\n",
    "    # 这什么意思？\n",
    "train_labels = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a469783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "把训练数据分成训练数据和验证数据\n",
    "\"\"\"\n",
    "\n",
    "# 验证数据\n",
    "valid_images, valid_labels = train_images[-10000:], train_labels[-10000:]\n",
    "# 训练数据\n",
    "train_images, train_labels = train_images[:1000], train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfafc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据中的自变量矩阵\n",
    "test_images = x_test/255\n",
    "# 测试数据的独热码\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,j in enumerate(y_test):\n",
    "    test_labels[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e454fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def relu2derive(x):\n",
    "    return (x>0)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp/np.sum(temp, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "799bd904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_section(layer, row_start, row_end, col_start, col_end):\n",
    "    section = layer[:,row_start:row_end, col_start:col_end]\n",
    "    return section.reshape(-1, 1, row_end-row_start, col_end-col_start)\n",
    "\n",
    "def conv_reshape(image, kernel_rows, kernel_cols):\n",
    "    image = np.pad(image, ((0,0),(1,1),(1,1)),mode='constant')\n",
    "    image_sections = []\n",
    "    for row_start in range(image.shape[1] - kernel_rows + 1):\n",
    "        for col_start in range(image.shape[2] - kernel_cols + 1):\n",
    "            image_sections.append(get_image_section(image, row_start, row_start+kernel_rows, col_start, col_start+kernel_cols))\n",
    "            # 为什么这里没有-1\n",
    "    expanded_input = np.concatenate(image_sections,axis=1)\n",
    "    es = expanded_input.shape\n",
    "    layer = expanded_input.reshape(es[0]*es[1],-1)\n",
    "    return layer\n",
    "\n",
    "def conv_2d(layer,kernels,bias):\n",
    "    layer = np.dot(layer,kernels) + bias\n",
    "    return relu(layer)\n",
    "\n",
    "def maxpool(layer, pool_rows, pool_cols, batch_size, input_rows, input_cols, num_kernels):\n",
    "    layer =layer.reshape(batch_size,input_rows, input_cols, num_kernels)\n",
    "    \n",
    "    layer_out_shape = (layer.shape[0], int(layer.shape[1]/2), int(layer.shape[2]/2), layer.shape[3])\n",
    "    layer_out = np.zeros(layer_out_shape)\n",
    "    argmax_out = []\n",
    "    # layer_2_drive = np.zeros(layer.shape)\n",
    "    for k in range(num_kernels):\n",
    "        layer_sections = []\n",
    "        for row_start in range(0, layer.shape[1] - pool_rows + 1, 2):\n",
    "            for col_start in range(0, layer.shape[2] - pool_cols +1, 2):\n",
    "                layer_sections.append(get_image_section(layer[:,:,:,k],row_start, row_start+pool_rows, col_start, col_start+pool_cols))\n",
    "        layer_temp = np.concatenate(layer_sections, axis=1)\n",
    "        layer_temp = layer_temp.reshape(layer_temp.shape[0]*layer_temp.shape[1], -1)\n",
    "        out = np.max(layer_temp,axis=1)\n",
    "        argmax_out.append(np.argmax(layer_temp, axis=1))\n",
    "        layer_out[:,:,:,k]=out.reshape(layer_out_shape[:-1])\n",
    "    return layer_out, argmax_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2db94853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_2_derive(delta, pool_argmax, pool_rows, pool_cols, batch_size, input_rows, input_cols, num_kernels):\n",
    "    \n",
    "    delta_conv = np.zeros((batch_size,input_rows,input_cols,num_kernels))\n",
    "    \n",
    "    after_pool_rows = int(input_rows/pool_rows)\n",
    "    after_pool_cols = int(input_cols/pool_cols)\n",
    "    \n",
    "    delta = delta.reshape(batch_size,after_pool_rows,after_pool_cols,num_kernels)\n",
    "    \n",
    "    for k in range(num_kernels):\n",
    "        delta_k = np.zeros((int(batch_size*after_pool_rows*after_pool_cols), pool_rows*pool_cols))\n",
    "        \n",
    "        delta_k[:,pool_argmax[k]] = delta[:,:,:,k].flatten()\n",
    "        delta_k = delta_k.re(batch_size, after_pool_rows*after_pool_cols, pool_rows*pool_cols)\n",
    "        delta_k_row = 0\n",
    "        for row_start in range(0, input_rows - pool_rows + 1, 2):\n",
    "            for col_start in range(0,input_cols - pool_cols + 1, 2):\n",
    "                delta_conv[:,row_start:(row_start+pool_rows),col_start:(col_start+pool_cols),k] = delta_k[:,delta_k_row,:].reshape(batch_size,pool_rows,pool_cols)\n",
    "                delta_k_row += 1\n",
    "    return delta_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bb11323",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epoches = 0.01, 1000\n",
    "pixels_per_image, num_labels = 784, 10\n",
    "\n",
    "input_rows, input_cols = 28, 28\n",
    "# 卷积层核的宽度高度和深度\n",
    "kernel_rows,kernel_cols, num_kernels = 3,3,32\n",
    "pool_rows,pool_cols=2,2\n",
    "FC_size=128\n",
    "\n",
    "batch_size = 100\n",
    "num_batch = int(len(train_images)/batch_size)\n",
    "\n",
    "hidden_maxpool = int((input_rows/2)*(input_cols/2)*num_kernels)\n",
    "\n",
    "\n",
    "b_kernels = np.zeros((1,num_kernels))  # 截距初始值全部为0\n",
    "b_2_3 = np.zeros((1,FC_size))          # b_2_3全部初始为0\n",
    "b_3_4 = np.zeros((1,num_labels))       # b_3_4全部初始为0\n",
    "\n",
    "# 卷积核的初始值\n",
    "kernels = 0.2*np.random.random((kernel_rows*kernel_cols, num_kernels)) - 0.1\n",
    "w_2_3 = 0.2*np.random.random((hidden_maxpool, FC_size)) - 0.1\n",
    "w_3_4 = 0.2 * np.random.random((FC_size , num_labels)) -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoches):\n",
    "    correct_cnt, val_correct_cnt = 0,0\n",
    "    loss, val_loss = 0.0,0.0\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        #----------------------------------------------#\n",
    "        # 正向传播算法\n",
    "        batch_start, batch_end = i*batch_size, (i-1)*batch_size\n",
    "        batch_image = train_images[batch_start:batch_end]\n",
    "        batch_label = train_labels[batch_start:batch_end]\n",
    "        layer_0 = conv_reshape(batch_image, kernel_rows, kernel_cols)\n",
    "        layer_1 = conv_2d(layer_0, kernels, b_kernels)\n",
    "        layer_1_shape = layer_1.shape\n",
    "        \n",
    "        layer_2, pool_argmax = maxpool(layer_1, pool_rows, pool_cols, batch_size, input_rows, input_cols, num_kernels)\n",
    "        layer_2 = layer_2.reshape(batch_size, -1)\n",
    "        \n",
    "        layer_3 = relu(np.dot(layer_2, w_2_3) + b_2_3)\n",
    "        \n",
    "        layer_4 = softmax(np.dot(layer_3, w_3_4) + b_3_4)\n",
    "        #-------------------------------------------------#\n",
    "        # 反向传播算法\n",
    "        for k in range(batch_size):\n",
    "            loss -= np.log(layer_4[k:k+1, np.argmax(batch_label[k:k+1])])\n",
    "            \n",
    "            correct_cnt += int(np.argmax(layer_4[k:k+1]) == np.argmax(batch_label[k:k+1]))\n",
    "            \n",
    "        layer_4_delta = (layer_4 - batch_label)/batch_size\n",
    "        layer_3_delta = np.dot(layer_4_delta, w_3_4.T)*relu2derive(layer_3)\n",
    "        \n",
    "        layer_2_delta = np.dot(layer_3_delta, w_2_3.T)\n",
    "        layer_2_delta = maxpool_2_derive(layer_2_delta, pool_argmax, pool_rows, pool_cols, batch_size, input_rows,input_cols,num_kernels)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.reshape(layer_1_shape)*relu2derive(layer_1)\n",
    "        \n",
    "        b_3_4 -=lr * np.sum(layer_4_delta, axis=0, keepdims=True)\n",
    "        b_2_3 -=lr * np.sum(layer_3_delta, axis=0, keepdims=True)\n",
    "        b_kernels -= lr * np.sum(layer_1_delta, axis=0, keepdims=True)/(input_rows*input_cols)\n",
    "        \n",
    "        w_3_4 -= lr * layer_3.T.dot(layer_4_delta)\n",
    "        w_2_3 -= lr * layer_2.T.dot(layer_3_delta)\n",
    "        kernels -= lr * layer_0.T.dot(layer_1_delta)/(input_rows * input_cols)\n",
    "        \n",
    "        #---------------------------------------------------------#\n",
    "        # 每训练100次，计算一次验证误差\n",
    "        if(epoch % 100 ==0 or epoch==epoches-1):\n",
    "            layer_0 = conv_reshape(valid_images, kernel_rows, kernel_cols)\n",
    "            layer_1 = conv_2d(layer_0, kernels, b_kernels)\n",
    "            layer_1 = layer_1.reshape(len(valid_images), -1)\n",
    "            \n",
    "            layer_2, pool_argmax  = maxpool(layer_1,pool_rows, pool_cols, len(valid_images), input_rows, input_cols, num_kernels)\n",
    "            \n",
    "            layer_3 = relu(np.dot(layer_2.reshape(len(valid_images),-1),w_2_3)+b_2_3)\n",
    "            \n",
    "            layer_4 = softmax(np.dot(layer_3, w_3_4) + b_3_4)\n",
    "            \n",
    "            for k in range(len(valid_images)):\n",
    "                val_loss -= np.log(layer_4[k:k+1,np.argmax(valid_labels[k:k+1])])\n",
    "                val_correct_cnt += int(np.argmax(layer_4[k:k+1]) == np.argmax(valid_labels[k:k+1]))\n",
    "                \n",
    "            print(\"e:%3d; Tr_Loss:%0.2f; Tr_Acc:%0.2f; Val_Loss: %0.2f; Val_Acc:%0.3f\"%(epoch, loss/float(len(train_images)),correct_cnt/float(len(train_images)), val_loss/float(len(train_images)), val_correct_cnt/float(len(train_images))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------#\n",
    "# 计算测量误差\n",
    "num_test_train = len(test_images)\n",
    "        \n",
    "layer_0 = conv_reshape(test_images, kernel_rows, kernel_cols)\n",
    "layer_1 = conv_2d(layer_0, kernels, b_kernels)\n",
    "layer_1 = layer_1.reshape(num_test_train,-1)\n",
    "        \n",
    "layer_2, pool_argmax = maxpool(layer_1, pool_rows, pool_cols, num_test_train, input_rows, input_cols, num_kernels)\n",
    "layer_2 = layer_2.reshape(num_test_train, -1)\n",
    "        \n",
    "layer_3 = relu(np.dot(layer_2, w_2_3) + b_2_3)\n",
    "layer_4 = softmax(np.dot(layer_3, w_3_4) + b_3_4)\n",
    "        \n",
    "test_loss, test_correct_cnt = 0, 0\n",
    "for k in range(num_test_train):\n",
    "    test_loss -= np.log(layer_4[k:k+1,np.argmax(test_labels[k:k+1])])\n",
    "            \n",
    "    test_correct_cnt += int(np.argmax(layer_4[k:k+1]) == np.argmax(test_labels[k:k+1]))\n",
    "            \n",
    "print(\"Test Loss: %0.3f Test Acc: %0.3f\"% (test_loss/num_test_train, test_correct_cnt/num_test_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
